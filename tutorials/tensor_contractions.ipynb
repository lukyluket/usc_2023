{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653c1817",
   "metadata": {},
   "source": [
    "# Generic tensor contractions\n",
    "\n",
    "This first part is Almost literally copied from  G. Evenbly tensors.net tutorial 1\n",
    "\n",
    "## Defining generic tensors  \n",
    "Here we will start studying generic tensor networks and their contraction.\n",
    "Part of any tensor network study is to understand how to contract some of the parts of the tensor networks, and eventually simplify them. \n",
    "\n",
    "The first task is to initialize tensors, we typically did this by creating vectors and reshaping them into higher dimensional tensors, today we will start with those higher dimensional tensors.\n",
    "Let' s create a random tensor with three legs (an order 4 tensor) with size 2, 3, 4, 5 and call it A, remember we always work with complex tensors in quantum mechanics\n",
    "and an order 3 tensor with size 4 5 6 and call it B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de370d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lets initialize some tensors in Python/Numpy\n",
    "import numpy as np\n",
    "# tensor with randomly generated entries, order 3, dims: 2-by-3-by-4\n",
    "\n",
    "A = np.random.rand(2,3,4,5) +1j*np.random.rand(2,3,4,5)\n",
    "B = np.random.rand(4,5,6) +1j*np.random.rand(4,5,6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8738da",
   "metadata": {},
   "source": [
    "Then we create an identity matrix that is 5 by 5 and call it C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b2333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity matrix, order 2, dims: 5-by-5 \n",
    "C = np.eye(5,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038b4ea",
   "metadata": {},
   "source": [
    "Other special tensors are those made by all ones, that clearly can be multiplied by an arbitrary random complex constant, thus obtaining a tensor where each element is the same, create one that has order 4 and dimension 2, 4, 2, 4, create it and call it D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a40856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of 1's, order 4, dims: 2-by-4-by-2-by-4\n",
    "D = (np.random.rand()+1j*np.random.rand())*np.ones((2,4,2,4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621280c1",
   "metadata": {},
   "source": [
    "Finally when a tensor only has few non-zero elements, one can create a tensor made of all zeros and fill the desider elements. E.g. create a tensor with order 2 made of zero and fill the first element with a random complex number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of 0's, order 2, dims: 3-by-5\n",
    "D = np.zeros((3,5))+1j* np.zeros((3,5))\n",
    "D[0,0] = np.random.rand()+1j*np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a1fb2",
   "metadata": {},
   "source": [
    "Now we can reorder the legs (permuting them which incurs in a computational cost proportional to the size of the tensor) or grouping or splitting the legs, which does not have a relevant computational cost (for large tensors) since it only changes the labels used to address the elements \n",
    "![reshape_permute](../pictures/reshape_permute.png \"from G. Evenbly\")\n",
    "For example implement the above permutation and reshaping  for the tensor A  and B defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atilda = A.transpose(3,0,1,2)\n",
    "Btilda = B.reshape(4,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b5a91",
   "metadata": {},
   "source": [
    "## Tensor contractions\n",
    "\n",
    "We now enter the realm of tensor contractions. First of all  remeber that contracting two tensors means summing the product of the tensor elements, it is a generalization of matrix multiplication\n",
    "\n",
    "$ M^i_k =\\sum _j A^i_j * B^j_k$, in the theory material you have gone through the diagramatic (or Penrose) notation for such operations. So let's put them in practice here. \n",
    "\n",
    "We will start by contracting two tensors. This can be done in several ways.\n",
    "Define a new tensor $B$ with order 4 and dimensions 3,4,2,5\n",
    "Now contract it with the $A$ tensor defined above on the second and fifth leg\n",
    "![Contraction between two tensors](../pictures/two_tensors_contract.png \"For G. Evenbly\")\n",
    "\n",
    "First compute it using for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(2,3,4,5) +1j*np.random.rand(2,3,4,5)\n",
    "B = np.random.rand(3,4,2,5) +1j*np.random.rand(3,4,2,5) \n",
    "C = np.zeros((A.shape[0],A.shape[2],B.shape[1],B.shape[2]))\n",
    "C = C+1j*C \n",
    "for i in range(A.shape[0]):\n",
    "    for m in range(A.shape[1]):\n",
    "        for j in range(A.shape[2]):\n",
    "            for n in range(A.shape[3]):\n",
    "                for k in range(B.shape[1]):\n",
    "                    for l in range(B.shape[2]):\n",
    "                        C[i,j,k,l] += A[i,m,j,n]*B[m,k,l,n]\n",
    "                       \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf13fa",
   "metadata": {},
   "source": [
    "Now repeat the same operation by transforming the two tensors into matrix and then performing a matrix multiplication, call the resulting tensor $\\tilde{C}$ and compare that the two methods provide the same result.\n",
    "![Contraction as matrix multiplication](../pictures/contraction_as_matrix_multiplication.png \"For G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap = A.transpose(0,2,1,3); \n",
    "Bp = B.transpose(0,3,1,2)\n",
    "App = Ap.reshape(A.shape[0]*A.shape[2],A.shape[1]*A.shape[3]);\n",
    "Bpp = Bp.reshape(B.shape[0]*B.shape[3],B.shape[1]*B.shape[2])\n",
    "Cpp = App @ Bpp;             \n",
    "C_tilde= Cpp.reshape(A.shape[0],A.shape[2],B.shape[1],B.shape[2])\n",
    "np.max(C-C_tilde)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4d314",
   "metadata": {},
   "source": [
    "## Computational cost of tensor contractions. \n",
    "\n",
    "As you have seen in the theory material, contracting tensors comes at a cost, here there is the summary of that cost is in this picture from tensors.net \n",
    "![Cost of contraction](../pictures/contraction_cost.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bd6d1",
   "metadata": {},
   "source": [
    "## More than 2 tensors \n",
    "When your network to contract include more than two tensors it is computationally advantageous to break the contraction into pairwise contractions. For example if you need to contract three tensors, and you do it in a single shot (by using for loops) you incur into a higher computational cost. Try it below follwing the diagarm. Contract it with for loops and by sequence of matrix multiplications.\n",
    "![three_tensors](../pictures/three_tensors.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8df526",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "A = np.random.rand(d,d) \n",
    "B = np.random.rand(d,d)\n",
    "C = np.random.rand(d,d)\n",
    "# Evaluare network via summation over internal indices\n",
    "F0 = np.zeros((d,d))\n",
    "for di in range(d):\n",
    "    for dj in range(d):\n",
    "        for dk in range(d):\n",
    "            for dl in range(d):\n",
    "                F0[di,dj] = F0[di,dj] + A[di,dk]*B[dk,dl]*C[dl,dj]\n",
    "            \n",
    "# Evaluare network via sequence of binary contractions\n",
    "F1 = (A @ B) @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d93eeb",
   "metadata": {},
   "source": [
    "In general the cost of the contraction of a tensor network depends on the order of the contraction. The picture below from tensors.net provide an explicit example \n",
    "![Cost of contraction](../pictures/order_cost.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070447bf",
   "metadata": {},
   "source": [
    "As an exercise find the optimal contraction sequence of this diagarm (from tensors.net) and write the code the performs the optimized contraction transforming the tensors to matrices and multiplying them.\n",
    " ![reshape_permute](../pictures/exercice.png \"from G. Evenbly\")\n",
    "  Initialize the tensors as random tensors whose legs all share the same dimension $d=10$ \n",
    "  use the following convention for the ordering of indices\n",
    "   ![reshape_permute](../pictures/oredering_indices.png \"from G. Evenbly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=10\n",
    "A = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "B = np.random.rand(d,d,d,d)+1j*np.random.rand(d,d,d,d)\n",
    "C = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "D = np.random.rand(d,d)+1j*np.random.rand(d,d)\n",
    "\n",
    "\n",
    "#First I contract C with D with cost d^4\n",
    "Cp = C.reshape(d,d*d)\n",
    "DC = (D@Cp).reshape(d,d,d)\n",
    "# Then I contrac B with DC this has computational cost d^5\n",
    "Bp = B.reshape(d*d,d*d)\n",
    "DCp= DC.transpose(1,0,2).reshape(d*d,d)\n",
    "BDC = (Bp@DCp).reshape(d,d,d)\n",
    "#At last I absorb A into the contraction with cost d^4\n",
    "BDCp = BDC.reshape(d,d*d)\n",
    "Ap=A.transpose(0,2,1)\n",
    "App=Ap.reshape(d*d,d)\n",
    "\n",
    "E =BDCp@App\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133c260-52e6-4e7f-8c90-2857a80a5c11",
   "metadata": {},
   "source": [
    "## Libraries to perform tensor contractions\n",
    "\n",
    "We are now in position to use libraries \n",
    "Let's start by redefining the tensors, aslo since here we are interested in optimizing the contraction also import the time package and the matplotlib to perform \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4afe03-4713-4661-85f6-2e493deeee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "d=10\n",
    "A = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "B = np.random.rand(d,d,d,d)+1j*np.random.rand(d,d,d,d)\n",
    "C = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "D = np.random.rand(d,d)+1j*np.random.rand(d,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914cd96-fdbd-473d-8689-3d31f52d5bb2",
   "metadata": {},
   "source": [
    "We now retake the computation of the last class and check that the leading cost is actually  $d^5$\n",
    " , we define the function contraction_by_hand, that take the five tensors and perform the contraction using only transpose and reshape as last time,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bde61e-2531-46dc-abf5-2aeee5473592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction_by_hand(A,B,C,D):\n",
    "    d = A.shape[0]\n",
    "    Cp = C.reshape(d,d*d)\n",
    "    DC = (D@Cp).reshape(d,d,d)\n",
    "# Then I contrac B with DC this has computational cost d^5\n",
    "    Bp = B.reshape(d*d,d*d)\n",
    "    DCp= DC.transpose(1,0,2).reshape(d*d,d)\n",
    "    BDC = (Bp@DCp).reshape(d,d,d)\n",
    "#At last I absorb A into the contraction with cost d^4\n",
    "    BDCp = BDC.reshape(d,d*d)\n",
    "    Ap=A.transpose(0,2,1)\n",
    "    App=Ap.reshape(d*d,d)\n",
    "    E=BDCp@App\n",
    "    return E, DC,BDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4997cb-7d76-4d3b-b08c-8f4302f4fd98",
   "metadata": {},
   "source": [
    "Now repeat the contraction for several d in the interval 2- collect the execution time and plot then in a log-log plot. (Why do we use a log-log plot?) On the x-axis put the bond dimension  d  and on the y-axis put the execution time  t\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10300f2c-ec68-460e-8662-fe4a99dac066",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_to_execute =[]\n",
    "dmin=20\n",
    "dmax=80\n",
    "for d in range(dmin,dmax+1):\n",
    "    A = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "    B = np.random.rand(d,d,d,d)+1j*np.random.rand(d,d,d,d)\n",
    "    C = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "    D = np.random.rand(d,d)+1j*np.random.rand(d,d)\n",
    "    t1 =time.time()\n",
    "    E = contraction_by_hand(A,B,C,D)\n",
    "    t2 =time.time()\n",
    "    times_to_execute.append(t2-t1)\n",
    "    \n",
    "plt.plot(np.log(range(dmin,dmax+1)),np.log(times_to_execute),label ='Actual computational time')\n",
    "plt.plot(np.log(np.linspace(dmin,dmax,dmax-dmin+1)),np.log(np.linspace(dmin,dmax,dmax-dmin+1)**5)-24, label ='Asimptotic prediction')      \n",
    "plt.grid('on')\n",
    "plt.xlabel('Bond dimension')\n",
    "plt.ylabel('Execution time')\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7246c-ca1c-4a0a-9d23-1fa842b24fc8",
   "metadata": {},
   "source": [
    "Now we define the same contraction but using ncon, and check that the result and scaling are the same than the contraction by hand (also check the intermediate steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8b397-939a-4c68-a91e-9ba3c4b19a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ncon import ncon\n",
    "d=10\n",
    "A = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "B = np.random.rand(d,d,d,d)+1j*np.random.rand(d,d,d,d)\n",
    "C = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "D = np.random.rand(d,d)+1j*np.random.rand(d,d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def contraction_with_einsum(A,B,C,D):\n",
    "    d = A.shape[0]\n",
    "    DC =np.einsum('ef,fgd->egd',D,C)\n",
    "# Then I contrac B with DC this has computational cost d^5\n",
    "    BDC =np.einsum('acge,egd->acd',B,DC)\n",
    "#At last I absorb A into the contraction with cost d^4\n",
    "    E = np.einsum('acd,cbd->ab',BDC,A)\n",
    "    return E, DC, BDC\n",
    "def contraction_with_ncon(A,B,C,D):\n",
    "    d = A.shape[0]\n",
    "    DCn=ncon([D,C],[[-1,1],[1,-2,-3]])\n",
    "# Then I contrac B with DC this has computational cost d^5\n",
    "    BDCn =ncon([B,DCn],[[-1,-2,1,2],[2,1,-3]])\n",
    "#At last I absorb A into the contraction with cost d^4\n",
    "    En =ncon([BDCn,A],[[-1,1,2],[1,-2,2]])\n",
    "    \n",
    "    return En, DCn, BDCn\n",
    "def contraction_with_ncon_one_line(A,B,C,D):\n",
    "    TensorArray = [A,B,C,D]\n",
    "    IndexArray = [[4,-2,5],\n",
    "    [-1,4,2,3],[1,2,5],[3,1]]\n",
    "    E = ncon(TensorArray,IndexArray)\n",
    "    return E\n",
    "\n",
    "E1,DC1,BDC1 = contraction_by_hand(A,B,C,D)\n",
    "E2,DC2,BDC2 =contraction_with_einsum(A,B,C,D)\n",
    "E3,DC3,BDC3 =contraction_with_ncon(A,B,C,D)\n",
    "E4 = contraction_with_ncon_one_line(A,B,C,D)\n",
    "print('Difference in first partial contraction')\n",
    "print(np.max(DC1-DC3))\n",
    "print('Difference in second partial contraction')\n",
    "print(np.max(BDC1-BDC3))\n",
    "print('Difference in total contraction')\n",
    "\n",
    "print(np.max(E1-E3))\n",
    "\n",
    "print(np.max(E1-E4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212a800-2a00-434e-a954-682130760799",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_to_execute_by_hand =[]\n",
    "times_to_execute_ncon =[]\n",
    "times_to_execute_ncon1 =[]\n",
    "times_to_execute_ein =[]\n",
    "\n",
    "for d in range(dmin,dmax+1):\n",
    "    A = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "    B = np.random.rand(d,d,d,d)+1j*np.random.rand(d,d,d,d)\n",
    "    C = np.random.rand(d,d,d)+1j*np.random.rand(d,d,d)\n",
    "    D = np.random.rand(d,d)+1j*np.random.rand(d,d)\n",
    "    \n",
    "    t1 =time.time()\n",
    "    E = contraction_by_hand(A,B,C,D)\n",
    "    t2 =time.time()\n",
    "    times_to_execute_by_hand.append(t2-t1)\n",
    "    \n",
    "    t1e =time.time()\n",
    "    E = contraction_with_einsum(A,B,C,D)\n",
    "    t2e =time.time()\n",
    "    times_to_execute_ein.append(t2e-t1e)\n",
    "    \n",
    "    t1n =time.time()\n",
    "    E = contraction_with_ncon(A,B,C,D)\n",
    "    t2n =time.time()\n",
    "    times_to_execute_ncon.append(t2n-t1n)\n",
    "    \n",
    "    t1n1 =time.time()\n",
    "    E = contraction_with_ncon_one_line(A,B,C,D)\n",
    "    t2n1 =time.time()\n",
    "    times_to_execute_ncon1.append(t2n1-t1n1)\n",
    "    \n",
    "plt.plot(np.log(range(dmin,dmax+1)),np.log(times_to_execute),label ='Actual computational time by hand')\n",
    "plt.plot(np.log(range(dmin,dmax+1)),np.log(times_to_execute_ein),label ='Actual computational time einsum')\n",
    "plt.plot(np.log(range(dmin,dmax+1)),np.log(times_to_execute_ncon),label ='Actual computational time ncon')\n",
    "plt.plot(np.log(range(dmin,dmax+1)),np.log(times_to_execute_ncon1),label ='Actual computational time ncon1')\n",
    "\n",
    "plt.plot(np.log(np.linspace(dmin,dmax,dmax-dmin+1)),np.log(np.linspace(dmin,dmax,dmax-dmin+1)**5)-24, label ='Asimptotic prediction')      \n",
    "plt.grid('on')\n",
    "plt.xlabel('Bond dimension')\n",
    "plt.ylabel('Execution time')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6896fea-0fc8-4e75-a3e7-8fd6379ae6ce",
   "metadata": {},
   "source": [
    "### Norm of an MPS\n",
    "Define the MPS by a list of $N$ three leg tensors (the boundary tensors only have 2 legs) with size $D,D,d$, order their leg as in the drawing,\n",
    "\n",
    "![](../pictures/mps_norm.png)\n",
    "\n",
    "Define a function that computes the norm of the MPS, (its leading cost should scale as $N d^2 D^3$)  and check it by running the code for different $D$ in the range $D=200-800$. Plot the log-log plot of the time it requires versus the bond dimension and plot the theoretical expectation, to check that your contractions performs as it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f9890-64f3-477d-821a-54407e592bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
